#### 15.2.2 RLHF中的具体实现——PPO算法
前面介绍了ChatGPT所使用的输出人类反馈行为的RLHF算法，可以看到直接使用人的偏好（或者说人的反馈）来对模型整体的输出结果计算Reward或Loss，显然要比传统的“给定上下文，预测下一个词”的损失函数合理得多。基于这个思想，ChatGPT的创造者提出了使用强化学习的方法，利用人类反馈信号直接优化语言模型。

在前面的章节中，我们完成了火箭降落任务，相信完成的读者会有一种强烈的自豪感。现在开始进行一项新的任务，即将PPO算法以RLHF的训练形式对ChatGPT进行微调。

在ChatGPT中，PPO算法用于训练机器人进行对话。通过训练机器人的Actor和Critic神经网络，机器人能够在对话中根据当前状态选择最优的回复，从而提高对话的质量。PPO算法使用一个改进的替代目标函数（Surrogate Objective Function）来更新Actor网络的参数，这个替代目标函数不但更快，而且更可靠，因此比其他基于梯度的强化学习算法更容易实现。此外，PPO算法还可以在训练过程中使用信任区域（Trust Region，对超过区域的值进行裁剪）方法来限制每次更新的幅度，以确保更新的稳定性，如图15 - 9所示。

![image](https://github.com/user-attachments/assets/a3e2fe3f-e549-4228-be5e-70e7cedd88dd)


|图15 - 9 RLHF对大语言模型训练的三个阶段|
|----|
| 图中展示了RLHF训练大语言模型的三个阶段：收集示范数据并训练监督策略、收集比较数据并训练奖励模型、使用PPO算法根据奖励模型优化策略  |

ChatGPT对大语言模型的训练可以具体分成以下几个步骤。

1. **定义环境和动作空间**

ChatGPT算法的环境包括用户输入和机器人回复。对于PPO算法，我们需要定义机器人的动作空间，即机器人可以采取的所有可能的操作。在这种情况下，机器人的动作可以是不同的回复，每个回复都有一个概率，这些概率可以表示为Softmax输出。

2. **定义策略网络和价值网络**

在PPO算法中，我们需要定义两个神经网络：一个是Actor网络，用于确定机器人的行为；另一个是Critic网络，用于评估Actor的性能。在ChatGPT中，我们可以使用预先训练的语言模型作为Actor和Critic网络。

3. **定义PPO的损失函数**

PPO算法使用一个改进的替代目标函数来更新Actor网络的参数。这个surrogate objective function包括两部分：一部分是ratio；另一部分是clipped surrogate objective。ratio是Actor网络新旧策略的比率，而clipped surrogate objective通过对ratio进行剪裁来确保更新的稳定性。

4. **使用PPO算法训练机器人**

在每个训练周期中，ChatGPT算法会根据当前的状态选择一个动作，并且根据选择的动作获取一个奖励。然后，使用PPO算法更新Actor和Critic网络的参数，以最大化累计奖励。更新过程中还需要使用信赖域（Trust Region）方法来限制每次更新的幅度，以确保更新的稳定性。

5. **重复训练直到收敛**

ChatGPT算法会一直重复训练机器人，直到机器人的性能收敛。在每个训练周期结束后，算法会评估机器人的性能，并将机器人的性能与之前的性能进行比较，以确定是否需要继续训练。

#### 15.3 基于RLHF实战的ChatGPT正向评论的生成
前面介绍过了，RLHF算法实际上是一种利用人类的反馈进行增强学习的方法，旨在使机器智能能够在不需要大量训练数据的情况下，从人类专家那里获得指导和改进。而PPO算法在RLHF中被广泛应用，本节进入RLHF的实战部分，实现我们自己的基于PPO算法的正向评论生成机器人。

在这里需要读者复习前面第13和第14章的内容，本节将重复使用和组合以往讲解过的知识，并依托GPT - 2语言模型作为我们的ChatGPT语言生成模型，这是因为如果选用更大的模型，可能性会好一些，但是一般家用计算机没有足够的运行空间，而我们是以学习为主，需要照顾更多的读者，因此这里采用较小的语言模型。有兴趣的读者可以在学完本章后自行尝试更大的语言模型。

在本书中已经完整实现了使用RLHF的ChatGPT模型框架，读者可以直接运行本书配套源码中的
ppo_sentiment_example.py，此实现代码较多，这里就不完整呈现了，只讲解部分重点内容。

# 完整代码
```python
import os

os.environ["CUDA_VISIBLE_DEVICES"] = "0"
import time
import random

import torch

from tqdm import tqdm
import numpy as np

from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline

#from trl.gpt2 import GPT2HeadWithValueModel
from RLHF.trl.ppo import PPOTrainer

config = {
    "model_name": 'uer/gpt2-chinese-cluecorpussmall',
    "steps": 25000,
    "batch_size": 128,
    "forward_batch_size": 16,
    "ppo_epochs": 4,   
    "lr": 2e-6,
    "init_kl_coef":0.2,
    "target": 6,
    "horizon":10000,
    "gamma":1,
    "lam":0.95,
    "cliprange": .2,
    "cliprange_value":.2,
    "vf_coef":.1,
    "gen_len": 16,
    "save_freq": 5,
}

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
pipe_device = 0 if torch.cuda.is_available() else -1

# prompt池
prompts = [
     '酒店',
     '周围',
     '位置',
     '前台'
]

# 情感分类模型
senti_tokenizer = AutoTokenizer.from_pretrained('uer/roberta-base-finetuned-jd-binary-chinese')

senti_model = AutoModelForSequenceClassification.from_pretrained('uer/roberta-base-finetuned-jd-binary-chinese')
sentiment_pipe = pipeline('sentiment-analysis', model=senti_model, tokenizer=senti_tokenizer, device=pipe_device)

# 文本生成模型
from moudle import model
gpt2_model = model.GPT2(use_rlhf=True)
gpt2_model_ref = model.GPT2(use_rlhf=True)
gpt2_tokenizer = AutoTokenizer.from_pretrained(config['model_name'])
gpt2_tokenizer.eos_token = gpt2_tokenizer.pad_token
gpt2_model.to(device)
gpt2_model_ref.to(device)

gen_kwargs = {
    "min_length":-1,
    "top_k": 0.0,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": gpt2_tokenizer.eos_token_id
}

# RL Trainer
ppo_trainer = PPOTrainer(gpt2_model, gpt2_model_ref, gpt2_tokenizer, **config)
total_ppo_epochs = int(np.ceil(config["steps"]/config['batch_size']))

import matplotlib.pyplot as plt
image_list = []
for epoch in tqdm(range(total_ppo_epochs)):
    logs, timing = dict(), dict()
    t0 = time.time()

    batch = {
        'tokens': [],
        'query': []
    }
    for _ in range(config['batch_size']):
        random_prompt = random.choice(prompts)                                  # 随机选择一个prompt
        tokens = gpt2_tokenizer.encode(random_prompt)
        batch['tokens'].append(tokens)
        batch['query'].append(random_prompt)
    query_tensors = [torch.tensor(t[:-1]).long().to(device) for t in batch["tokens"]]

    t = time.time()
    response_tensors = []
    for i in range(config['batch_size']):
        gen_len = config['gen_len']
        prompt_token = query_tensors[i].detach().cpu().numpy()

        response = gpt2_model.generate(prompt_token = prompt_token,       # generate()用于直接生成token_id
                                       continue_buildingsample_num=gen_len)
        response_tensors.append(torch.tensor(response[-gen_len:]).to(device))   #这里输出的是一系列的Token，长度为gen_len
    batch['response'] = [gpt2_tokenizer.decode(r) for r in response_tensors]
    timing['time/get_response'] = time.time() - t

    t = time.time()
    texts = [q + r for q,r in zip(batch['query'], batch['response'])]           # 计算正向/负向情感得分
    pipe_outputs = sentiment_pipe(texts)
    rewards = []
    for output in pipe_outputs:
        if output['label'] == 'positive (stars 4 and 5)':
            rewards.append(output['score'])
        elif output['label'] == 'negative (stars 1, 2 and 3)':
            rewards.append(1 - output['score'])
        else:
            raise ValueError(f"错误的推理结果{output['label']}.")
    rewards = torch.tensor(rewards).to(device)                                  # 将正向情感的得分作为生成得分
    timing['time/get_sentiment_preds'] = time.time() - t

    t = time.time()
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)          # PPO Update

    mean_reward = torch.mean(rewards).cpu().numpy()
    image_list.append(mean_reward)
    print()
    print(f"epoch {epoch} mean-reward: {mean_reward}",'Random Sample 5 text(s) of model output:')
    for i in range(5):                                                           # 随机打5个生成的结果
        print(f'{i+1}. {random.choice(texts)}')

print(image_list)
plt.plot(image_list)
plt.show()

torch.save(gpt2_model.state_dict(),"./checkpoints/gpt2_model.pth")
torch.save(gpt2_model_ref.state_dict(),"./checkpoints/gpt2_model_ref.pth")


```
##### 15.3.1 RLHF模型进化的总体讲解

在第13章中已经详细介绍了PPO算法，并且在第14章完成了一个GPT - 2模型，可以自由生成对关键词prompt的描述文本。下面我们基于前面讲解的内容，实现一个基于中文情感识别模型的正向评论生成机器人。

这里需要说明的是，对于任何GPT系列的模型，其文本的生成形式都是相通的。读者可以自行替换合适的语言模型。

回忆第14章的算法模型GPT - 2，通过对其进行评论训练，使用一小段文本提示（prompt），模型就能够继续生成一段文字，如图15 - 10所示。

![image](https://github.com/user-attachments/assets/22fae26e-123b-40c9-aa9f-c1587b14b1c8)

|图15 - 10 使用文本提示继续生成一段文字|
|----|
| 图中展示了一段关于酒店的文本描述  |

但是这段评论生成的只是简单的文本描述，当前的GPT模型是不具备情绪识别能力的，如上面的生成结果都不符合正面情绪。这不能达到我们所需要的既定目标，即通过一定的训练使得模型生成具有正向情感评论的功能。对此的解决办法就是通过RLHF的方法来进化现有GPT模型，使其学会尽可能生成正向情感的评论。

具体而言，就是在每个模型根据文本提示生成一个结果时，我们需要反馈这个模型输出结果的得分是多少，即为模型的每个生成结果打分，图15 - 11展示了生成过程。

![image](https://github.com/user-attachments/assets/1f4e77b6-325d-422e-8e0b-ba4d06a058c4)


|图15 - 11 生成过程|
|----|
| 图中展示了模型输出的几条文本及平均奖励值  |

可以看到，随着模型的输出，为了简单起见，这里计算了评价均值作为反馈的分值，将训练评价的结果以图形的形式展示出来，评分结果如图15 - 12所示。

|图15 - 12 评分结果|
|----|
| 图中展示了评分随训练进行的波动折线图  |


从图5 - 12可以看到，随着训练的进行，正向评价分数也随之增加，基本上可以认为我们的训练是正确的。

![image](https://github.com/user-attachments/assets/0720c786-4a50-48b5-8b7a-563f0ca2a3b6)


##### 15.3.2 ChatGPT评分模块简介
前面介绍了ChatGPT的基本内容，本小节介绍所使用的评分模块。在这里我们使用Huggingface提供的中文二分类情感分类模型，基于网络评论数据集训练，能够对句子的评论情感进行判别，如图15 - 13所示。

|图15 - 13 对句子的评论情感进行判别|
|----|
| 图中展示了一个文本输入框及对应的正向、负向评论得分输出  |


![image](https://github.com/user-attachments/assets/916b144a-5937-4102-a11a-8cc6ae8b8ee2)


可以看到，在这里输入评论，其下方会输出对该评论的评分值，其中的positive为正向评论得分，而negative是负向评论得分。

既然使用的是基于Huggingface的评论模型，下面直接采用本地化的方法将模型部署在本地机器上，代码如下：
```python
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline

pipe_device = 0 if torch.cuda.is_available() else -1
# 情感分类模型
senti_tokenizer = AutoTokenizer.from_pretrained('uer/roberta-base-finetuned-jd-binary-chinese')
senti_model = AutoModelForSequenceClassification.from_pretrained('uer/roberta-base-finetuned-jd-binary-chinese')
sentiment_pipe = pipeline('sentiment-analysis', model=senti_model, tokenizer=senti_tokenizer, device=pipe_device)

text = ["这家店东西很好吃，但是饮料不怎么样。", "这家店的东西很好吃，我很喜欢，推荐！"]

result = sentiment_pipe(text[0])
print(text[0], result)
print("---------------------")
result = sentiment_pipe(text[1])
print(text[1], result)
```
输出结果如图15 - 14所示。

![image](https://github.com/user-attachments/assets/b591c3e3-849b-46af-95b0-624aaa6af064)


|图15 - 14 输出结果|
|----|
| 图中展示了两条文本及其对应的正向情感评分结果  |

从结果中可以看到，此时的输出只显示正向情感评分，而score就是具体的分值。


这里有个提示，关于反馈函数的设定并不是唯一的，读者在有条件的情况下，可以直接使用OpenAI提供的ChatGPT接口，通过拼接合适的提示词来获取更准确的评分。

##### 15.3.3 带有评分函数的ChatGPT模型的构建
本小节回到GPT模型，回忆第14章实现的可进行再训练的GPT模型，其中forward部分只输出了模型预测的logits，但是根据前面的讲解，相对于一般的GPT模型，还需要一个评分网络来接收对模型的评价反馈。

在这里可以简单地使用一个全连接层来完成此项评分功能，代码如下：
```python
value_layer = torch.nn.Sequential(torch.nn.Linear(768,1),torch.nn.Tanh(),torch.nn.Dropout(0.1))
...
output = embedding
value = self.value_layer(output)
value = torch.squeeze(value,dim=-1)
return logits,value
```
可以看到，此时通过对模型的输出进行反馈，从而调整模型的整体输出，而此时的输入embedding就是由GPT - 2模型的主体计算得到的。完整的GPT - 2模型如下：
```python
import torch
from torch.nn.parameter import Parameter
from transformers import BertTokenizer, GPT2Model
tokenizer = BertTokenizer.from_pretrained("uer/gpt2-chinese-cluecorpussmall")

class GPT2(torch.nn.Module):
    def __init__(self,use_rlhf = False):
        super().__init__()
        self.use_rlhf = use_rlhf
        #with torch.no_grad():
        self.model = GPT2Model.from_pretrained("uer/gpt2-chinese-cluecorpussmall")
        self.lm_head = torch.nn.Linear(768,21128,bias=False)
        weight = torch.load("./dataset/lm_weight.pth")
        self.lm_head.weight = Parameter(weight)
        self.value_layer = torch.nn.Sequential(torch.nn.Linear(768,1),torch.nn.Tanh(),torch.nn.Dropout(0.1))

    def forward(self,token_inputs):
        embedding = self.model(token_inputs)
        embedding = embedding["last_hidden_state"]
        embedding = torch.nn.Dropout(0.1)(embedding)
        logits = self.lm_head(embedding)

        if not self.use_rlhf:
            return logits
        else:
            output = embedding
            value = self.value_layer(output)
            value = torch.squeeze(value,dim=-1)
            return logits,value

    @torch.no_grad()
    def generate(self, continue_building_sample_num, prompt_token=None, temperature=1., top_p=0.95):
        """
        :param continue_building_sample_num: 这个参数指的是在输入的prompt_token后再输出多少个字符
        :param prompt_token: 这是需要转换成Token的内容，这里需要输入一个list
        :param temperature: 
        :param top_k: 
        :return: 输出一个Token序列
        """
        prompt_token_new = list(prompt_token) # 使用这行代码，在生成的Token里面有102个分隔符
        for i in range(continue_building_sample_num):
            _token_inp = torch.tensor([prompt_token_new]).to("cuda")
            if self.use_rlhf:
                result, _ = self.forward(_token_inp)
            else:
                result = self.forward(_token_inp)
            logits = result[:, -1, :]
            probs = torch.softmax(logits / temperature, dim=-1)
            next_token = self.sample_top_p(probs, top_p) # 预设的top_p = 0.95
            next_token = next_token.reshape(-1)
            prompt_token_new.append(next_token.item())
        return prompt_token_new

    def sample_top_p(self, probs, p):
        probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)
        probs_sum = torch.cumsum(probs_sort, dim=-1)
        mask = probs_sum - probs_sort > p
        probs_sort[mask] = 0.0
        probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))
        next_token = torch.multinomial(probs_sort, num_samples=1)
        next_token = torch.gather(probs_idx, -1, next_token)
        return next_token
```

##### 15.3.4 RLHF中的PPO算法——KL散度
本小节依次讲解在训练时使用的PPO2模型，相对于第13章讲解的PPO算法，实际上还需要active与reward方法。因此，在具体使用时，我们采用两个相同的GPT - 2模型分别作为算法的实施与更新模块，代码如下：
```python
from moudle import model
gpt2_model = model.GPT2(use_rlhf=True)
gpt2_model_ref = model.GPT2(use_rlhf=True)
```
这是我们已定义好的GPT - 2模型。为了简单起见，我们使用的均为带有评分函数的GPT - 2模型。
下面对PPO整体模型进行介绍，在这里我们采用自定义的PPOTrainer类来对模型进行整体操作，简单的代码如下：
```python
ppo_trainer = PPOTrainer(gpt2_model, gpt2_model_ref, gpt2_tokenizer, **config)
``` 
