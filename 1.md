### 第15章 实战训练自己的ChatGPT
在2023年年初，OpenAI推出了一种大型语言模型，名为ChatGPT。这个模型最大的特点是可以像聊天机器人一样进行对话。与其他语言模型不同的是，ChatGPT经过微调，能够以对话的方式执行各种任务，比如回答查询、解决编码、制定营销计划、解决数学问题等。
用户只需使用电子邮件地址创建OpenAI账户，登录ChatGPT并输入查询，就可以与这个模型进行对话。ChatGPT可以用自然人类语言回答用户的问题，如果回答不满足用户的需求，可以微调输入查询，直到获得预期的结果为止。
对于用户来说，ChatGPT是一个非常有用的工具，能够以更自然的方式与其互动，提高他们的参与度和满意度。ChatGPT的Logo如图15 - 1所示。
|图15 - 1 ChatGPT的Logo|
|----|
| 图中展示了OpenAI和ChatGPT的标识  |
本章将以实战为主，介绍ChatGPT模型训练的主要方法——RLHF（Reinforcement Learning from Human Feedback，人工强化学习反馈），并通过这个方法以GPT - 2模型为基础训练我们自己的ChatGPT。

#### 15.1 什么是ChatGPT
ChatGPT是一款基于人工智能技术驱动的自然语言处理工具，于2022年11月30日在美国发布。它通过学习和理解人类的语言来进行对话，还能根据聊天的上下文进行互动，让用户感觉像在和真人聊天交流一样。除聊天外，ChatGPT还可以完成撰写邮件、视频脚本、文案、翻译、代码等任务，如图15 - 2所示。

![image](https://github.com/user-attachments/assets/ea029778-fc89-4665-98eb-918b315aeb9a)


|图15 - 2 ChatGPT可以完成的任务|
|----|
| 图中展示了从Transformer到GPT，再到GPT - 2、GPT - 3，最后到ChatGPT的发展流程，标注了各阶段特点  |

ChatGPT系列也是逐步从一个简单的语言模型发展起来的。相较于之前的人工智能技术，ChatGPT的最大不同在于其具备承认自身错误、质疑用户提问时预设的错误条件，并且拒绝不当请求的能力。这种智能化程度让ChatGPT可以更好地为用户提供服务和帮助，如图15 - 3所示。

![image](https://github.com/user-attachments/assets/794273af-c414-4d47-bd9f-9947b57b04e3)


|图15 - 3 由ChatGPT生成的ChatGPT简介|
|----|
| 图中展示了一段ChatGPT生成的关于自身介绍的文本框，包含功能、应用领域等介绍  |

可以看到，ChatGPT的应用场景非常广泛，除了用来开发聊天机器人、编写和调试计算机程序外，还可以应用于文学、媒体相关领域的创作。例如，ChatGPT可以用鲁迅的文风进行文字创作，用Twitter的高级数据工程师的口吻给马斯克写周报等。ChatGPT在教育、考试、回答测试问题方面的表现也非常优秀，甚至在某些测试情境下表现得比普通人类测试者更好。

#### 15.2 RLHF模型简介
近年来，深度生成模型在生成结果的评估方面一直存在主观性和上下文依赖性的问题。现有的模型通常采用预测下一个单词的方式和简单的损失函数（如交叉熵）来建模，没有显式地引入人的偏好和主观意见。例如，我们希望模型生成一个有创意的故事、一段真实的信息性文本或者可执行的代码片段，这些结果难以用现有的、基于规则的文本生成指标来衡量。

如果我们使用生成文本的人工反馈作为性能衡量标准，或者进一步将该反馈用作损失来优化模型，这种方法也是可行的。这就是RLHF的思想：使用强化学习的方式直接优化带有人工反馈的语言模型。RLHF使得在一般文本数据语料库上训练的语言模型能够和复杂的人类价值观对齐。

早期，RLHF主要被应用在游戏、机器人等领域，在2019年以后，RLHF与语言模型相结合的工作开始陆续出现，如图15 - 4所示。其中，OpenAI的InstructGPT是一个重要的里程碑式的成果，现在被誉为ChatGPT的兄弟模型。不过，当时并非只有OpenAI在关注RLHF，DeepMind其实也关注到这一发展方向，先后发表了Gopher、Critic和Sparrow两个基于RLHF训练的语言模型，前者是一个问答模型，后者是一个对话模型，可惜效果不够惊艳。

![image](https://github.com/user-attachments/assets/d7ae59d1-5521-4c50-9399-5df3a3df0b73)


|图15 - 4 结合了RLHF的语言模型|
|----|
| 图中展示了“Reinforcement Learning from Human Feedback A step by step intro to RLHF”相关内容及示意图  |

OpenAI推出的ChatGPT对话模型掀起了新的AI热潮，它面对多种多样的问题对答如流，似乎已经打破了机器和人的边界。这一工作的背后是大型语言模型（Large Language Model, LLM）生成领域的新训练范式：RLHF，即以强化学习方式依据人类反馈优化语言模型。

##### 15.2.1 RLHF技术分解
在ChatGPT中，RLHF是一个复杂的概念，涉及多个模型和不同的训练阶段。为了更好地理解RLHF，我们可以将其分解为以下3个步骤（见图15 - 5）：
- 预训练语言模型（Language Model, LM）。
- 聚合问答数据并训练奖励模型（Reward Model, RM）。
- 使用强化学习（Reinforcement Learning, RL）对LM进行微调。
|图15 - 5 RLHF微调语言模型的三个步骤|
|----|
| 图中展示了生成监督微调模型、训练奖励模型、优化ChatGPT的流程，包括各步骤的具体操作  |

![image](https://github.com/user-attachments/assets/31ae5d08-8bcf-4ea0-aa36-3216ac36e679)


接下来，我们分别讲解这三个步骤。

1. **基于监督学习的预训练语言模型**

首先，我们使用经典的预训练目标来训练一个语言模型。在OpenAI发布的第一个RLHF模型InstructGPT中，使用了GPT - 3的较小版本，参数约为1700亿个。
然后，使用额外的文本或条件对这个语言模型进行微调，例如使用OpenAI对“更可取”（Preferable）的人工生成本文进行微调，如图15 - 6所示。

![image](https://github.com/user-attachments/assets/2ef3337f-dc90-411f-9d64-392776fcc4db)


|图15 - 6 对人工生成本文进行微调|
|----|
| 图中展示了从“Prompts & Text Dataset”和“Human Augmented Text (Optional)”输入到“Initial Language Model”的微调流程  |

接下来，我们基于LM来生成训练奖励模型（Reward Model, RM，也叫偏好模型）的数据，并在这一步引入人类的偏好信息。

2. **训练奖励模型**

RM的训练是RLHF的关键步骤。该模型接收一系列文本并返回一个标量奖励，用于量化人类的偏好。这个过程可以使用端到端方式进行LM建模，也可以使用模块化的系统进行建模（例如，对输出进
行排名，然后将排名转换为奖励）。而奖励数值的准确性对于RLHF对模型的反馈至关重要，如图15 - 7所示。

|图15 - 7 训练RLHF奖励模型的流程|
|----|
| 图中展示了从“Prompts Dataset”输入，经“Initial Language Model”生成文本，再经“Human Scoring”后训练“Reward (Preference) Model”的流程  |

![image](https://github.com/user-attachments/assets/0af101b4-bc24-433e-a910-21bec206599e)


3. **使用强化学习进行微调**

长期以来，出于工程和算法的原因，人们认为用强化学习训练LM是不可能的。但是强化学习策略PPO（Proximal Policy Optimization，近端策略优化）算法的出现改变了这种情况。PPO算法确定的奖励函数的具体计算步骤（见图15 - 8）说明如下：

（1）将提示输入初始语言模型和当前微调的LM，分别得到输出文本，将来自当前策略的文本传递给RM得到一个标量的奖励。

（2）将两个模型生成的文本进行比较，计算差异的惩罚项，这被设计为输出词分布序列之间的KL（Kullback - Leibler）散度的缩放，之后将其用于惩罚RL策略，在每个训练批次中大幅偏离初
始模型，以确保模型输出合理连贯的文本。

（3）最后根据PPO算法，按当前批次数据的奖励指标进行优化（来自PPO算法on - policy的特性），其使用梯度约束确保更新步骤不会破坏学习过程的稳定性。

![image](https://github.com/user-attachments/assets/3e0d45d4-59e0-4cb0-9ea6-df3ae384ffa0)


|图15 - 8 基于RLHF的语言模型优化|
|----|
| 图中展示了从“Prompts Dataset”输入，经“Initial Language Model”和“Tuned Language Model (RL w/ PPO)”，结合“Reward (Preference) Model”进行“Reinforcement Learning Update (e.g. PPO)”的优化流程  | 
