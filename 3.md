下面对相对简单的散度计算函数AdaptiveKLController进行讲解（PPO2算法）。需要注意的是，无论是在经典的PPO算法还是我们自定义的PPO算法中，KL散度的计算都是一项重要的内容，它是一种用来描述两个分布之间距离的性能指标。

这里使用AdaptiveKLController来实现模型计算，这种方法会在梯度函数中添加clip操作，称为PPO2算法。

其实现原理是，当优势函数的值为正，即需要加强对当前动作的选择概率时，将会对两分布在当前状态和动作下的比值的最大值进行约束。如果最大值超过阈值，则停止对策略的更新；当优势函数的值为负，即需要减小对当前动作的选择概率时，将会对两分布在当前状态和动作下的比值的最小值进行约束，如果最小值超过阈值，也会停止对策略的更新。通过这种方式，可以实现在参数更新的同时保证两分布之间的距离在设定的范围内，如图15 - 15所示。

![image](https://github.com/user-attachments/assets/db1e0350-c2da-4154-af51-1edb9c79c93d)


|图15 - 15 PPO2算法演示|
|----|
| 图中展示了PPO2算法的公式及优势函数A大于0和小于0时的约束情况  |

这种方法使得模型能够通过动态调整KL约束项的惩罚系数，来达到约束参数更新幅度的目的，即参数的更新应尽可能小以保证训练的稳定性，但同时应在分布空间更新得足够大以使策略分布发生改变。

如图15 - 16所示的算法，对于更新前后的KL距离，我们设定一个目标约束值target（一个可调整的超参数），直接设置KL散度的最大更新约束值。

但是，和使用KL散度的约束值不同的是，该方法对优势函数做了限制。其中，当重要性采样的系数大于或小于一个固定值（一般设置区间范围为[-0.2,0.2]，见下面的代码部分）时，该更新会被忽略，即裁剪后的损失不依赖于参数，所以不产生任何梯度信息。本质上是忽略了差异过大的新策略所产生的优势函数值，保证了训练的稳定性和梯度更新的单调递增所需的步幅小的要求。

![image](https://github.com/user-attachments/assets/fbeab4b9-6c74-40f8-a03d-8a5a5d0bde31)


|图15 - 16 对KL散度有约束的PPO优化策略|
|----|
| 图中展示了对KL散度有约束的PPO优化策略的算法流程  |

在模型中的具体实现如下，读者可以对照验证：
```python
class AdaptiveKLController:
    def __init__(self, init_kl_coef, target, horizon):
        self.value = init_kl_coef
        self.target = target
        self.horizon = horizon

    def update(self, current, n_steps):
        target = self.target
        proportional_error = np.clip(current / target - 1, -0.2, 0.2)
        mult = 1 + proportional_error * n_steps / self.horizon
        self.value *= mult
```

##### 15.3.5 RLHF中的PPO算法——损失函数

应用RLHF的目的是最大限度反馈生成模型的奖励值，但同时希望生成模型的输出在经过PPO算法的反馈
后，不要距离原本的模型生成结果太远。因此，需要使用不同的损失函数来对反馈结果进行约束。

完成此项工作的是PPO算法中的损失函数，如同我们在前面介绍的一样，PPO算法中的损失函数是通过比较当前策略与旧策略之间的差异来计算的，以确保更新不会太大，从而避免策略迭代过程中的过度拟合问题。

在此处损失函数的实现如下：
```python
def loss(self, old_logprobs, values, rewards, query, response, model_input):
    """Calculate policy and value losses."""
    lastgaelam = 0
    advantages_reversed = []
    gen_len = response.shape[1]

    for t in reversed(range(gen_len)):
        nextvalues = values[:, t + 1] if t < gen_len - 1 else 0.0
        delta = rewards[:, t] + self.ppo_params['gamma'] * nextvalues - values[:, t]
        lastgaelam = delta + self.ppo_params['gamma'] * self.ppo_params['lam'] * lastgaelam
        advantages_reversed.append(lastgaelam)
    advantages = torch.stack(advantages_reversed[::-1]).transpose(0, 1)

    returns = advantages + values  # (batch, generated_seq_len)
    advantages = whiten(advantages)
    advantages = advantages.detach()

    logits, vpred = self.model(model_input)  # logits -> (batch, all_seq_len, vocab_size); vpred -> (batch, all_seq_len)
    logprob = logprobs_from_logits(logits[:, -gen_len-1:, :], model_input[:, 1:])

    #only the generation part of the values/logprobs is needed
    logprob, vpred = logprob[:, -gen_len:], vpred[:, -gen_len-1:-1]  # logprob -> (batch, generated_seq_len); vpred -> (batch, generated_seq_len)

    vpredclipped = clip_by_value(vpred, values - self.ppo_params["cliprange_value"],
                                 values + self.ppo_params["cliprange_value"])

    vf_losses1 = (vpred - returns)**2  # value loss = v - (r + gamma * n_next)
    vf_losses2 = (vpredclipped - returns)**2  # value loss clipped
    vf_loss =.5 * torch.mean(torch.max(vf_losses1, vf_losses2))
    vf_clipfrac = torch.mean(torch.gt(vf_losses2, vf_losses1).double())

    ratio = torch.exp(logprob - old_logprobs)
    pg_losses = -advantages * ratio  # importance sampling
    pg_losses2 = -advantages * torch.clamp(ratio, 1.0 - self.ppo_params['cliprange'],
                                            1.0 + self.ppo_params['cliprange'] )

    pg_loss = torch.mean(torch.max(pg_losses, pg_losses2))
    pg_clipfrac = torch.mean(torch.gt(pg_losses2, pg_losses).double())

    loss = pg_loss + self.ppo_params['vf_coef'] * vf_loss

    entropy = torch.mean(entropy_from_logits(logits))
    approxxkl =.5 * torch.mean((logprob - old_logprobs)**2)
    policykl = torch.mean(logprob - old_logprobs)
    return_mean, return_var = torch.mean(returns), torch.var(returns)
    value_mean, value_var = torch.mean(values), torch.var(values)

    stats = dict(
        loss=dict(policy=pg_loss, value=vf_loss, total=loss),
        policy=dict(entropy=entropy, approxxkl=approxxkl, policykl=policykl, clipfrac=pg_clipfrac,
                    advantages=advantages, advantages_mean=torch.mean(advantages), ratio=ratio),
        return=dict(mean=return_mean, var=return_var),
        val=dict(vpred=torch.mean(vpred), error=torch.mean((vpred - returns) ** 2), clipfrac=vf_clipfrac, mean=value_mean, var=value_var),
    )

    return pg_loss, self.ppo_params['vf_coef'] * vf_loss, flatten_dict(stats)
```

### 15.4 本章小结

本章展示了使用RLHF进行自己的ChatGPT实战训练，限于目前只是进行讲解和演示，使用了GPT - 2模型进行主模型的调配，同时使用了Huggingface的中文二分类情感分类模型对结果进行评判。

这种方式的好处是可以很简易地进行模型训练，但是难点在于创建的反馈模型无法较好地反映人类的真实情感。此时还有一种较好的且具有一定可行性的训练方案，就是使用OpenAI提供的ChatGPT作为反馈模型，设定专业的关键词Prompt进行打分测试，从而完成模型的训练。

从第16章开始，我们将使用真正意义上的大模型，以带有70亿参数的清华大学开源ChatGLM为例，向读者介绍大型模型的微调和继续训练等工作。 
